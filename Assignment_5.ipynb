{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSn380sZe708CYX/OiS+So",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vignesh424/NLTK/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS2hScaWc8OQ"
      },
      "source": [
        "#import \n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet as wn\n",
        "from operator import itemgetter\n",
        "from timeit import Timer"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "typ-hIcHw6t3",
        "outputId": "4197306e-14c0-4170-92eb-7037fdfca113"
      },
      "source": [
        "nltk.download(\"all\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO6zvik6d3Yv"
      },
      "source": [
        " **1. Find out more about sequence objects using Python's help facility. In the interpreter, type help(str), help(list) , and help(tuple) . This will give you a full list of the functions supported by each type. Some functions have special names flanked with underscore; as the help documentation shows, each such function corresponds to something more familiar. For example x.__getitem__(y) is just a long-winded way of saying x[y] .**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFVqiVfAd2cb",
        "outputId": "d1d142ba-4529-4a45-fbd2-d91023c41054"
      },
      "source": [
        "help(str)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class str in module builtins:\n",
            "\n",
            "class str(object)\n",
            " |  str(object='') -> str\n",
            " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
            " |  \n",
            " |  Create a new string object from the given object. If encoding or\n",
            " |  errors is specified, then the object must expose a data buffer\n",
            " |  that will be decoded using the given encoding and error handler.\n",
            " |  Otherwise, returns the result of object.__str__() (if defined)\n",
            " |  or repr(object).\n",
            " |  encoding defaults to sys.getdefaultencoding().\n",
            " |  errors defaults to 'strict'.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __format__(self, format_spec, /)\n",
            " |      Return a formatted version of the string as described by format_spec.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(self, key, /)\n",
            " |      Return self[key].\n",
            " |  \n",
            " |  __getnewargs__(...)\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __hash__(self, /)\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mod__(self, value, /)\n",
            " |      Return self%value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __rmod__(self, value, /)\n",
            " |      Return value%self.\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  __sizeof__(self, /)\n",
            " |      Return the size of the string in memory, in bytes.\n",
            " |  \n",
            " |  __str__(self, /)\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  capitalize(self, /)\n",
            " |      Return a capitalized version of the string.\n",
            " |      \n",
            " |      More specifically, make the first character have upper case and the rest lower\n",
            " |      case.\n",
            " |  \n",
            " |  casefold(self, /)\n",
            " |      Return a version of the string suitable for caseless comparisons.\n",
            " |  \n",
            " |  center(self, width, fillchar=' ', /)\n",
            " |      Return a centered string of length width.\n",
            " |      \n",
            " |      Padding is done using the specified fill character (default is a space).\n",
            " |  \n",
            " |  count(...)\n",
            " |      S.count(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the number of non-overlapping occurrences of substring sub in\n",
            " |      string S[start:end].  Optional arguments start and end are\n",
            " |      interpreted as in slice notation.\n",
            " |  \n",
            " |  encode(self, /, encoding='utf-8', errors='strict')\n",
            " |      Encode the string using the codec registered for encoding.\n",
            " |      \n",
            " |      encoding\n",
            " |        The encoding in which to encode the string.\n",
            " |      errors\n",
            " |        The error handling scheme to use for encoding errors.\n",
            " |        The default is 'strict' meaning that encoding errors raise a\n",
            " |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
            " |        'xmlcharrefreplace' as well as any other name registered with\n",
            " |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
            " |  \n",
            " |  endswith(...)\n",
            " |      S.endswith(suffix[, start[, end]]) -> bool\n",
            " |      \n",
            " |      Return True if S ends with the specified suffix, False otherwise.\n",
            " |      With optional start, test S beginning at that position.\n",
            " |      With optional end, stop comparing S at that position.\n",
            " |      suffix can also be a tuple of strings to try.\n",
            " |  \n",
            " |  expandtabs(self, /, tabsize=8)\n",
            " |      Return a copy where all tab characters are expanded using spaces.\n",
            " |      \n",
            " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
            " |  \n",
            " |  find(...)\n",
            " |      S.find(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the lowest index in S where substring sub is found,\n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Return -1 on failure.\n",
            " |  \n",
            " |  format(...)\n",
            " |      S.format(*args, **kwargs) -> str\n",
            " |      \n",
            " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
            " |      The substitutions are identified by braces ('{' and '}').\n",
            " |  \n",
            " |  format_map(...)\n",
            " |      S.format_map(mapping) -> str\n",
            " |      \n",
            " |      Return a formatted version of S, using substitutions from mapping.\n",
            " |      The substitutions are identified by braces ('{' and '}').\n",
            " |  \n",
            " |  index(...)\n",
            " |      S.index(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the lowest index in S where substring sub is found, \n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Raises ValueError when the substring is not found.\n",
            " |  \n",
            " |  isalnum(self, /)\n",
            " |      Return True if the string is an alpha-numeric string, False otherwise.\n",
            " |      \n",
            " |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
            " |      there is at least one character in the string.\n",
            " |  \n",
            " |  isalpha(self, /)\n",
            " |      Return True if the string is an alphabetic string, False otherwise.\n",
            " |      \n",
            " |      A string is alphabetic if all characters in the string are alphabetic and there\n",
            " |      is at least one character in the string.\n",
            " |  \n",
            " |  isascii(self, /)\n",
            " |      Return True if all characters in the string are ASCII, False otherwise.\n",
            " |      \n",
            " |      ASCII characters have code points in the range U+0000-U+007F.\n",
            " |      Empty string is ASCII too.\n",
            " |  \n",
            " |  isdecimal(self, /)\n",
            " |      Return True if the string is a decimal string, False otherwise.\n",
            " |      \n",
            " |      A string is a decimal string if all characters in the string are decimal and\n",
            " |      there is at least one character in the string.\n",
            " |  \n",
            " |  isdigit(self, /)\n",
            " |      Return True if the string is a digit string, False otherwise.\n",
            " |      \n",
            " |      A string is a digit string if all characters in the string are digits and there\n",
            " |      is at least one character in the string.\n",
            " |  \n",
            " |  isidentifier(self, /)\n",
            " |      Return True if the string is a valid Python identifier, False otherwise.\n",
            " |      \n",
            " |      Use keyword.iskeyword() to test for reserved identifiers such as \"def\" and\n",
            " |      \"class\".\n",
            " |  \n",
            " |  islower(self, /)\n",
            " |      Return True if the string is a lowercase string, False otherwise.\n",
            " |      \n",
            " |      A string is lowercase if all cased characters in the string are lowercase and\n",
            " |      there is at least one cased character in the string.\n",
            " |  \n",
            " |  isnumeric(self, /)\n",
            " |      Return True if the string is a numeric string, False otherwise.\n",
            " |      \n",
            " |      A string is numeric if all characters in the string are numeric and there is at\n",
            " |      least one character in the string.\n",
            " |  \n",
            " |  isprintable(self, /)\n",
            " |      Return True if the string is printable, False otherwise.\n",
            " |      \n",
            " |      A string is printable if all of its characters are considered printable in\n",
            " |      repr() or if it is empty.\n",
            " |  \n",
            " |  isspace(self, /)\n",
            " |      Return True if the string is a whitespace string, False otherwise.\n",
            " |      \n",
            " |      A string is whitespace if all characters in the string are whitespace and there\n",
            " |      is at least one character in the string.\n",
            " |  \n",
            " |  istitle(self, /)\n",
            " |      Return True if the string is a title-cased string, False otherwise.\n",
            " |      \n",
            " |      In a title-cased string, upper- and title-case characters may only\n",
            " |      follow uncased characters and lowercase characters only cased ones.\n",
            " |  \n",
            " |  isupper(self, /)\n",
            " |      Return True if the string is an uppercase string, False otherwise.\n",
            " |      \n",
            " |      A string is uppercase if all cased characters in the string are uppercase and\n",
            " |      there is at least one cased character in the string.\n",
            " |  \n",
            " |  join(self, iterable, /)\n",
            " |      Concatenate any number of strings.\n",
            " |      \n",
            " |      The string whose method is called is inserted in between each given string.\n",
            " |      The result is returned as a new string.\n",
            " |      \n",
            " |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
            " |  \n",
            " |  ljust(self, width, fillchar=' ', /)\n",
            " |      Return a left-justified string of length width.\n",
            " |      \n",
            " |      Padding is done using the specified fill character (default is a space).\n",
            " |  \n",
            " |  lower(self, /)\n",
            " |      Return a copy of the string converted to lowercase.\n",
            " |  \n",
            " |  lstrip(self, chars=None, /)\n",
            " |      Return a copy of the string with leading whitespace removed.\n",
            " |      \n",
            " |      If chars is given and not None, remove characters in chars instead.\n",
            " |  \n",
            " |  partition(self, sep, /)\n",
            " |      Partition the string into three parts using the given separator.\n",
            " |      \n",
            " |      This will search for the separator in the string.  If the separator is found,\n",
            " |      returns a 3-tuple containing the part before the separator, the separator\n",
            " |      itself, and the part after it.\n",
            " |      \n",
            " |      If the separator is not found, returns a 3-tuple containing the original string\n",
            " |      and two empty strings.\n",
            " |  \n",
            " |  replace(self, old, new, count=-1, /)\n",
            " |      Return a copy with all occurrences of substring old replaced by new.\n",
            " |      \n",
            " |        count\n",
            " |          Maximum number of occurrences to replace.\n",
            " |          -1 (the default value) means replace all occurrences.\n",
            " |      \n",
            " |      If the optional argument count is given, only the first count occurrences are\n",
            " |      replaced.\n",
            " |  \n",
            " |  rfind(...)\n",
            " |      S.rfind(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the highest index in S where substring sub is found,\n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Return -1 on failure.\n",
            " |  \n",
            " |  rindex(...)\n",
            " |      S.rindex(sub[, start[, end]]) -> int\n",
            " |      \n",
            " |      Return the highest index in S where substring sub is found,\n",
            " |      such that sub is contained within S[start:end].  Optional\n",
            " |      arguments start and end are interpreted as in slice notation.\n",
            " |      \n",
            " |      Raises ValueError when the substring is not found.\n",
            " |  \n",
            " |  rjust(self, width, fillchar=' ', /)\n",
            " |      Return a right-justified string of length width.\n",
            " |      \n",
            " |      Padding is done using the specified fill character (default is a space).\n",
            " |  \n",
            " |  rpartition(self, sep, /)\n",
            " |      Partition the string into three parts using the given separator.\n",
            " |      \n",
            " |      This will search for the separator in the string, starting at the end. If\n",
            " |      the separator is found, returns a 3-tuple containing the part before the\n",
            " |      separator, the separator itself, and the part after it.\n",
            " |      \n",
            " |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
            " |      and the original string.\n",
            " |  \n",
            " |  rsplit(self, /, sep=None, maxsplit=-1)\n",
            " |      Return a list of the words in the string, using sep as the delimiter string.\n",
            " |      \n",
            " |        sep\n",
            " |          The delimiter according which to split the string.\n",
            " |          None (the default value) means split according to any whitespace,\n",
            " |          and discard empty strings from the result.\n",
            " |        maxsplit\n",
            " |          Maximum number of splits to do.\n",
            " |          -1 (the default value) means no limit.\n",
            " |      \n",
            " |      Splits are done starting at the end of the string and working to the front.\n",
            " |  \n",
            " |  rstrip(self, chars=None, /)\n",
            " |      Return a copy of the string with trailing whitespace removed.\n",
            " |      \n",
            " |      If chars is given and not None, remove characters in chars instead.\n",
            " |  \n",
            " |  split(self, /, sep=None, maxsplit=-1)\n",
            " |      Return a list of the words in the string, using sep as the delimiter string.\n",
            " |      \n",
            " |      sep\n",
            " |        The delimiter according which to split the string.\n",
            " |        None (the default value) means split according to any whitespace,\n",
            " |        and discard empty strings from the result.\n",
            " |      maxsplit\n",
            " |        Maximum number of splits to do.\n",
            " |        -1 (the default value) means no limit.\n",
            " |  \n",
            " |  splitlines(self, /, keepends=False)\n",
            " |      Return a list of the lines in the string, breaking at line boundaries.\n",
            " |      \n",
            " |      Line breaks are not included in the resulting list unless keepends is given and\n",
            " |      true.\n",
            " |  \n",
            " |  startswith(...)\n",
            " |      S.startswith(prefix[, start[, end]]) -> bool\n",
            " |      \n",
            " |      Return True if S starts with the specified prefix, False otherwise.\n",
            " |      With optional start, test S beginning at that position.\n",
            " |      With optional end, stop comparing S at that position.\n",
            " |      prefix can also be a tuple of strings to try.\n",
            " |  \n",
            " |  strip(self, chars=None, /)\n",
            " |      Return a copy of the string with leading and trailing whitespace removed.\n",
            " |      \n",
            " |      If chars is given and not None, remove characters in chars instead.\n",
            " |  \n",
            " |  swapcase(self, /)\n",
            " |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
            " |  \n",
            " |  title(self, /)\n",
            " |      Return a version of the string where each word is titlecased.\n",
            " |      \n",
            " |      More specifically, words start with uppercased characters and all remaining\n",
            " |      cased characters have lower case.\n",
            " |  \n",
            " |  translate(self, table, /)\n",
            " |      Replace each character in the string using the given translation table.\n",
            " |      \n",
            " |        table\n",
            " |          Translation table, which must be a mapping of Unicode ordinals to\n",
            " |          Unicode ordinals, strings, or None.\n",
            " |      \n",
            " |      The table must implement lookup/indexing via __getitem__, for instance a\n",
            " |      dictionary or list.  If this operation raises LookupError, the character is\n",
            " |      left untouched.  Characters mapped to None are deleted.\n",
            " |  \n",
            " |  upper(self, /)\n",
            " |      Return a copy of the string converted to uppercase.\n",
            " |  \n",
            " |  zfill(self, width, /)\n",
            " |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
            " |      \n",
            " |      The string is never truncated.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  maketrans(x, y=None, z=None, /)\n",
            " |      Return a translation table usable for str.translate().\n",
            " |      \n",
            " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
            " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
            " |      Character keys will be then converted to ordinals.\n",
            " |      If there are two arguments, they must be strings of equal length, and\n",
            " |      in the resulting dictionary, each character in x will be mapped to the\n",
            " |      character at the same position in y. If there is a third argument, it\n",
            " |      must be a string, whose characters will be mapped to None in the result.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOt27QjoeFFn",
        "outputId": "4c4bbc3b-ce0a-46b1-81bf-5584a4629c67"
      },
      "source": [
        "help(list)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class list in module builtins:\n",
            "\n",
            "class list(object)\n",
            " |  list(iterable=(), /)\n",
            " |  \n",
            " |  Built-in mutable sequence.\n",
            " |  \n",
            " |  If no argument is given, the constructor creates a new empty list.\n",
            " |  The argument must be an iterable if specified.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __delitem__(self, key, /)\n",
            " |      Delete self[key].\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(...)\n",
            " |      x.__getitem__(y) <==> x[y]\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __iadd__(self, value, /)\n",
            " |      Implement self+=value.\n",
            " |  \n",
            " |  __imul__(self, value, /)\n",
            " |      Implement self*=value.\n",
            " |  \n",
            " |  __init__(self, /, *args, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __reversed__(self, /)\n",
            " |      Return a reverse iterator over the list.\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  __setitem__(self, key, value, /)\n",
            " |      Set self[key] to value.\n",
            " |  \n",
            " |  __sizeof__(self, /)\n",
            " |      Return the size of the list in memory, in bytes.\n",
            " |  \n",
            " |  append(self, object, /)\n",
            " |      Append object to the end of the list.\n",
            " |  \n",
            " |  clear(self, /)\n",
            " |      Remove all items from list.\n",
            " |  \n",
            " |  copy(self, /)\n",
            " |      Return a shallow copy of the list.\n",
            " |  \n",
            " |  count(self, value, /)\n",
            " |      Return number of occurrences of value.\n",
            " |  \n",
            " |  extend(self, iterable, /)\n",
            " |      Extend list by appending elements from the iterable.\n",
            " |  \n",
            " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
            " |      Return first index of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  insert(self, index, object, /)\n",
            " |      Insert object before index.\n",
            " |  \n",
            " |  pop(self, index=-1, /)\n",
            " |      Remove and return item at index (default last).\n",
            " |      \n",
            " |      Raises IndexError if list is empty or index is out of range.\n",
            " |  \n",
            " |  remove(self, value, /)\n",
            " |      Remove first occurrence of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  reverse(self, /)\n",
            " |      Reverse *IN PLACE*.\n",
            " |  \n",
            " |  sort(self, /, *, key=None, reverse=False)\n",
            " |      Stable sort *IN PLACE*.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __hash__ = None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6W4aeHSeGp2",
        "outputId": "ffe7effd-dd40-41c0-c34f-aafe6a68dddc"
      },
      "source": [
        "help(tuple)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class tuple in module builtins:\n",
            "\n",
            "class tuple(object)\n",
            " |  tuple(iterable=(), /)\n",
            " |  \n",
            " |  Built-in immutable sequence.\n",
            " |  \n",
            " |  If no argument is given, the constructor returns an empty tuple.\n",
            " |  If iterable is specified the tuple is initialized from iterable's items.\n",
            " |  \n",
            " |  If the argument is a tuple, the return value is the same object.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, value, /)\n",
            " |      Return self+value.\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(self, key, /)\n",
            " |      Return self[key].\n",
            " |  \n",
            " |  __getnewargs__(self, /)\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __hash__(self, /)\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __repr__(self, /)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  count(self, value, /)\n",
            " |      Return number of occurrences of value.\n",
            " |  \n",
            " |  index(self, value, start=0, stop=9223372036854775807, /)\n",
            " |      Return first index of value.\n",
            " |      \n",
            " |      Raises ValueError if the value is not present.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(*args, **kwargs) from builtins.type\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWA2Yxg5eh-U"
      },
      "source": [
        "**2. Identify three operations that can be performed on both tuples and lists. Identify three list operations that cannot be performed on tuples. Name a context where using a list instead of a tuple generates a Python error.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY6wq06geaW1",
        "outputId": "67a675ab-140b-404e-8c55-4b6fdd99080d"
      },
      "source": [
        "#define list \n",
        "a=[1,2,3,4,5,6,7,7]\n",
        "#first operation is length , second operation is minimum and third operation is maximum \n",
        "print(\"Length :\",len(a))\n",
        "print(\"Minimum:\",min(a))\n",
        "print(\"Maximum:\",max(a))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length : 8\n",
            "Minimum: 1\n",
            "Maximum: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gpg3Qf1fMk1",
        "outputId": "5ee9b037-94bf-42f0-bcd6-6ec0f7bbdce6"
      },
      "source": [
        "#define tuple\n",
        "a=(1,2,3,4,5,6,7,7)\n",
        "#first operation is length , second operation is minimum and third operation is maximum \n",
        "print(\"Length :\",len(a))\n",
        "print(\"Minimum:\",min(a))\n",
        "print(\"Maximum:\",max(a))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length : 8\n",
            "Minimum: 1\n",
            "Maximum: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c-L8osM9iKs_",
        "outputId": "b2f3e3a0-6ab5-4657-a274-bc772c9264f6"
      },
      "source": [
        "#3 operations performed with list and not tuple are 1.reverse 2.insert 3. delete\n",
        "a=(1,2,3,4)\n",
        "'''a.reverse()\n",
        "a.append(3)\n",
        "a.pop()'''"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a.reverse()\\na.append(3)\\na.pop()'"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3AXhiwagEH0",
        "outputId": "f6d348ed-bd71-4701-e6b7-3b470b1167e6"
      },
      "source": [
        "b=[1,2,3,4]\n",
        "#reverse\n",
        "b.reverse()\n",
        "print(b)\n",
        "#append\n",
        "b.append(3)\n",
        "print(b)\n",
        "#pop\n",
        "b.pop()\n",
        "print(b)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 3, 2, 1]\n",
            "[4, 3, 2, 1, 3]\n",
            "[4, 3, 2, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qitQqCk0gr20",
        "outputId": "a407cbea-e45b-45ba-b88c-8f281b77178c"
      },
      "source": [
        "tuple_only = 'natural', ['N', 'AE1', 'CH', 'ER0', 'AH0', 'L']\n",
        "print(tuple_only)\n",
        "# though it seems okay to create a list with ['natural', ['N', 'AE1', 'CH', 'ER0', 'AH0', 'L']]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('natural', ['N', 'AE1', 'CH', 'ER0', 'AH0', 'L'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAUzJ8YajCEn"
      },
      "source": [
        "**3. Find out how to create a tuple consisting of a single item. There are at least two ways to do this.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5ZSYNwpi549",
        "outputId": "851b1bb0-ab0e-4533-eb1d-cc1184862e58"
      },
      "source": [
        "#tuple creation with single item 1st method\n",
        "a= 'single',\n",
        "b= tuple(a)\n",
        "print(b)\n",
        "#tuple creation with single item 2nd method\n",
        "c=['double']\n",
        "d=tuple(c)\n",
        "print(d)\n",
        "print(type(b))\n",
        "print(type(d))\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('single',)\n",
            "('double',)\n",
            "<class 'tuple'>\n",
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBShSCAokzrS"
      },
      "source": [
        "**4. Create a list words = ['is', 'NLP', 'fun', '?'] . Use a series of assignment statements (e.g. words[1] = words[2] ) and a temporary variable tmp to transform this list into the list ['NLP', 'is', 'fun', '!'] . Now do the same transformation using tuple assignment.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ1yPYUtk6mA",
        "outputId": "e336ea20-f4ce-4c34-9c12-0aa879f40b96"
      },
      "source": [
        "words = ['is', 'NLP', 'fun', '?']\n",
        "words[0], words[1] = words[1], words[0]     # tmp is not necessary\n",
        "words[-1] = '!'\n",
        "words"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'is', 'fun', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haWwunBOk9Gp",
        "outputId": "e3b3afbc-f5e6-42fb-b9b8-979fc10179be"
      },
      "source": [
        "words_tuple = 'is', 'NLP', 'fun', '?'\n",
        "words_new = words_tuple[1], words_tuple[0], words_tuple[2], '!'\n",
        "list(words_new)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP', 'is', 'fun', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4AVIChJj_hU"
      },
      "source": [
        "**5. Read about the built-in comparison function cmp , by typing help(cmp) . How does it differ in behavior from the comparison operators?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_UniMFkQRd"
      },
      "source": [
        "Well, cmp() compare the two objects x and y and return an integer according to the outcome. The return value is negative if x < y, zero if x == y and strictly positive if x > y.\n",
        "However, it was deprecated in Python3 and cmp is no longer a built-in comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj6D40KwkUZa"
      },
      "source": [
        "**6. Does the method for creating a sliding window of n-grams behave correctly for the two limiting cases: n = 1, and n = len(sent)?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-e9QQVBjOBs",
        "outputId": "03a33dab-93e4-4431-c568-4482b46726de"
      },
      "source": [
        "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
        "n = len(sent)     # n = 1\n",
        "[sent[i:i+n] for i in range(len(sent)-n+1)]\n",
        "# Here The two boundary cases both works."
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'dog', 'gave', 'John', 'the', 'newspaper']]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeascR7ClR5F"
      },
      "source": [
        "**7. We pointed out that when empty strings and empty lists occur in the condition part of an if clause, they evaluate to False . In this case, they are said to be occurring in a Boolean context. Experiment with different kind of non-Boolean expressions in Boolean contexts, and see whether they evaluate as True or False .**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JNJ39ZMlT3L",
        "outputId": "ca894110-9a29-4840-d02b-a7da5895805b"
      },
      "source": [
        "#Let us assume nodes of mesh topology as n and take value as 4\n",
        "n=4\n",
        "a = (n*(n-1)/2)\n",
        "if a==6:\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print('False')\n",
        "\n",
        "\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNxO8zmcl4UR"
      },
      "source": [
        "False, None, numeric zero of all types, and empty strings and containers (including strings, tuples, lists, dictionaries, sets and frozensets). All other values are interpreted as true."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pYAImk3l_6-"
      },
      "source": [
        "**8. Use the inequality operators to compare strings, e.g. 'Monty' < 'Python' . What happens when you do 'Z' < 'a' ? Try pairs of strings which have a common prefix, e.g. 'Monty' < 'Montague' . Read up on \"lexicographical sort\" in order to understand what is going on here. Try comparing structured objects, e.g. ('Monty', 1) < ('Monty', 2) . Does this behave as expected?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V6NZuDJku1f",
        "outputId": "eb656231-14ce-4b95-b99b-8f1a6b1cf420"
      },
      "source": [
        "print('Monty' < 'Python')        # 77 ASCII value of ('M') and 80 of ('P')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5cnFa9omTWn",
        "outputId": "b614672a-5edc-43cc-9078-fdb7b68cb016"
      },
      "source": [
        "print('Z' < 'a')                  # 90 ASCII value of ('Z') and 97 of ('a')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0A5vO95mVqH",
        "outputId": "d5e5b136-48bc-4295-8dbe-c4d0b931c6dd"
      },
      "source": [
        "print('Monty' < 'Montague')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt_v0ZcVpHtq"
      },
      "source": [
        "**Why the answer is False ??** \n",
        "\n",
        "Here is a simple trick.\n",
        "Consider 2 words \"Monty\" and only \"Monta\". Please do not take full word \"Montague\" here. You will get to know in a minute.\n",
        "\n",
        "The sum of ASCII characters of \"Mont\" is , say 20(I know it is more than that but let us consider 20 for now), and if we add ASCII value of \"y\" then it becomes 20+121=141. On the other hand, if we add ASCII value of \"a\" then it becomes 20+97=117. Here 117 <141. \n",
        "So, Monty is larger than Montague."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bXHbcUUmYy2",
        "outputId": "9e13ad30-638a-4a57-ba13-b60109d86e3b"
      },
      "source": [
        "print(('Monty', 1) < ('Monty', 2))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deNsMrHrnaOE"
      },
      "source": [
        "**Why True ?**\n",
        "\n",
        "Here is another simple trick.\n",
        "Consider numbers 1 and 2 respectively from the individual tuples. Now, you know 1 is lesser than 2. So, ('Monty',2) is greater than ('Monty',1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePRSHxHdrEuL"
      },
      "source": [
        "**9. Write code that removes whitespace at the beginning and end of a string, and normalizes whitespace between words to be a single space character.**\n",
        "\n",
        "1. **do this task using split() and join()**\n",
        "2. **do this task using regular expression substitutions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tAFqSmqvnOEV",
        "outputId": "cd1e2397-b07a-42b2-bc45-5241c1ef35b4"
      },
      "source": [
        "s = ' I am \\n a simple\\t boy. '\n",
        "' '.join(s.split())"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am a simple boy.'"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Evp2iiFIr4pd",
        "outputId": "3501c872-09e8-449d-a8bb-522f73e8d087"
      },
      "source": [
        "import re\n",
        "s_ = re.sub(r'^\\s|\\s$', '', s)       # remove whitespace at the beginning and end of a string\n",
        "re.sub(r'\\s+', ' ', s_)              # normalize whitespace between words"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am a simple boy.'"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dznUKEGSsARR"
      },
      "source": [
        "**10. Write a program to sort words by length. Define a helper function cmp_len which uses the cmp comparison function on word lengths.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEWca0oDr7GN",
        "outputId": "43020547-58f2-4700-bbfd-7239dbd69338"
      },
      "source": [
        "word_list = 'If you smell what The Rock is cooking'.split()\n",
        "sorted(word_list, key=len)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If', 'is', 'you', 'The', 'what', 'Rock', 'smell', 'cooking']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df39aAFasj-i"
      },
      "source": [
        "\n",
        "**11. Create a list of words and store it in a variable sent1 . Now assign sent2 = sent1 . Modify one of the items in sent1 and verify that sent2 has changed.**\n",
        "\n",
        "**a. Now try the same exercise but instead assign sent2 = sent1[:] . Modify sent1 again and see what happens to sent2 .**\n",
        "\n",
        "\n",
        "**b. Now define text1 to be a list of lists of strings (e.g. to represent a text consisting of multiple sentences. Now assign text2 = text1[:] , assign a new value to one of the words, e.g. text1[1][1] = 'Monty' . Check what this did to text2.**\n",
        "\n",
        "\n",
        "**c. Load Python's deepcopy() function (i.e. from copy import deepcopy ), consult its documentation, and test that it makes a fresh copy of any object.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTRNQe6nsGPc",
        "outputId": "f5460cec-8801-47ad-80bb-9046b75ef6b6"
      },
      "source": [
        "sent1 = ['a', 'list', 'of', 'word']\n",
        "sent2 = sent1\n",
        "sent1[3] = 'words'\n",
        "sent2"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'list', 'of', 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI6rmP9YtG-n",
        "outputId": "0a8a3bb1-5a0a-412d-df21-7fc3bba98685"
      },
      "source": [
        "sent2 = sent1[:]\n",
        "sent1[3] = 'Word'\n",
        "sent2\n",
        "\n",
        "# sent2 = sent1[:] makes a copy of each element of sent1.\n",
        "# Since the elements are type of string,\n",
        "# the copy is just copy by value.\n",
        "# So the modification of sent1 doesn't affect sent2"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'list', 'of', 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ99OvYNtJKm",
        "outputId": "f7cbb3d4-ba39-4a98-be13-aefde5bcebb0"
      },
      "source": [
        "text1 = [\"Hush, little baby, don't say a word,\".split(),\n",
        "         \"Papa's going to buy you a mockingbird.\".split(),\n",
        "         \"If that mockingbird won't sing,\".split(),\n",
        "         \"Papa's going to buy you a diamond ring.\".split(),\n",
        "         \"If that diamond ring turns to brass,\".split(),\n",
        "         \"Papa's going to buy you a looking-glass.\".split(),\n",
        "         \"If that looking-glass gets broke,\".split(),\n",
        "         \"Papa's going to buy you a billy-goat.\".split(),\n",
        "         \"If that billy-goat runs away,\".split(),\n",
        "         \"Papa's going to buy you another today.\".split()]\n",
        "text2 = text1[:]\n",
        "text1[1][1] = 'Monty'\n",
        "text2\n",
        "\n",
        "# text2 = text1[:] makes a copy of each element of sent1\n",
        "# The elements are lists and lists are objects.\n",
        "# Therefore, the copy is copy by reference.\n",
        "# The modification of text1 will affect text2 as well."
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hush,', 'little', 'baby,', \"don't\", 'say', 'a', 'word,'],\n",
              " [\"Papa's\", 'Monty', 'to', 'buy', 'you', 'a', 'mockingbird.'],\n",
              " ['If', 'that', 'mockingbird', \"won't\", 'sing,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'diamond', 'ring.'],\n",
              " ['If', 'that', 'diamond', 'ring', 'turns', 'to', 'brass,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'looking-glass.'],\n",
              " ['If', 'that', 'looking-glass', 'gets', 'broke,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'billy-goat.'],\n",
              " ['If', 'that', 'billy-goat', 'runs', 'away,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'another', 'today.']]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkYAyVWYtjAr",
        "outputId": "8b14b360-9062-4c17-a22c-44cfb008c16e"
      },
      "source": [
        "# https://docs.python.org/3/library/copy.html\n",
        "from copy import deepcopy\n",
        "text3 = deepcopy(text1)\n",
        "text1[1][1] = 'going'\n",
        "text3\n",
        "\n",
        "# the modification of original list won't affect the copy"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Hush,', 'little', 'baby,', \"don't\", 'say', 'a', 'word,'],\n",
              " [\"Papa's\", 'Monty', 'to', 'buy', 'you', 'a', 'mockingbird.'],\n",
              " ['If', 'that', 'mockingbird', \"won't\", 'sing,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'diamond', 'ring.'],\n",
              " ['If', 'that', 'diamond', 'ring', 'turns', 'to', 'brass,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'looking-glass.'],\n",
              " ['If', 'that', 'looking-glass', 'gets', 'broke,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'a', 'billy-goat.'],\n",
              " ['If', 'that', 'billy-goat', 'runs', 'away,'],\n",
              " [\"Papa's\", 'going', 'to', 'buy', 'you', 'another', 'today.']]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbur994qzRoj"
      },
      "source": [
        "**12. Initialize an $n$-by-$m$ list of lists of empty strings using list multiplication, e.g. word_table = [[''] * n] * m.** \n",
        "\n",
        "**What happens when you set one of its values, e.g. word_table[1][2] = \"hello\"?**\n",
        "\n",
        "**Explain why this happens. Now write an expression using range() to construct a list of lists, and show that it does not have this problem.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBcJybx7zlO2",
        "outputId": "a4d2ddfb-c712-4c1e-cc1d-023bf66e8e77"
      },
      "source": [
        "m, n = 6, 7\n",
        "\n",
        "word_table = [[''] * n] * m\n",
        "print(word_table)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', '']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSW_poVrzs90",
        "outputId": "2beaad69-a678-4c54-fc4f-741c0dc5e7ee"
      },
      "source": [
        "word_table[1][2] = (\"hello\")\n",
        "print(word_table)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['', '', 'hello', '', '', '', ''], ['', '', 'hello', '', '', '', ''], ['', '', 'hello', '', '', '', ''], ['', '', 'hello', '', '', '', ''], ['', '', 'hello', '', '', '', ''], ['', '', 'hello', '', '', '', '']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQqx1htf0I-2",
        "outputId": "dea355fa-2b39-4b8e-f6de-becf2b48e31b"
      },
      "source": [
        "\n",
        "word_table = [['' for i in range(n)] for j in range(m)]\n",
        "print(word_table)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', '']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up0dlUXR0Nqo",
        "outputId": "36a40258-cee7-4372-cadc-533ef39d6030"
      },
      "source": [
        "word_table[1][2] = (\"hello\")\n",
        "print(word_table)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['', '', '', '', '', '', ''], ['', '', 'hello', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', ''], ['', '', '', '', '', '', '']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaN2D4bv0lFx"
      },
      "source": [
        "\n",
        "In this case, we're creating a brand new empty string for each of the iterations through $n$, and likewise through $m$. As the strings are not copies of each other, changes in one are not reflected in the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaIIwO0It9Ur"
      },
      "source": [
        "**13. Write code to initialize a two-dimensional array of sets called word_vowels and process a list of words, adding each word to word_vowels[l][v] where l is the length of the word and v is the number of vowels it contains.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR2qLcMztnFK"
      },
      "source": [
        "n = 10\n",
        "word_vowels = [[set() for _ in range(n)] \n",
        "for _ in range(n)]\n",
        "word_list = 'Write code to initialize an array and process a list of words'.split()\n",
        "for word in word_list:\n",
        "    l = len(word)\n",
        "    v = sum(1 for letter in word.lower() if letter in 'aeiou')\n",
        "    if l < n:                                # in case the length of word is larger than the array size n\n",
        "        word_vowels[l][v].add(word)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nTOISEBuXuf"
      },
      "source": [
        "**14. Write a function novel10(text) that prints any word that appeared in the last 10% of a text that had not been encountered earlier.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSNRKZOruOk5"
      },
      "source": [
        "def novel10(text):\n",
        "    novels = set()\n",
        "    text_list = text.split()\n",
        "    text_len = len(text_list)\n",
        "    for word in text_list[int(0.9 * text_len):]:\n",
        "        if word not in text_list[:int(0.9 * text_len)]:\n",
        "            novels.add(word)\n",
        "        for word in novels:\n",
        "          print(word)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3tktQXW0tG8"
      },
      "source": [
        "**15. Write a program that takes a sentence expressed as a single string, splits it and counts up the words. Get it to print out each word and the word's frequency, one per line, in alphabetical order.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joZE6pIc0_pA"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def print_words_and_frequency(text):\n",
        "    \"\"\"\n",
        "    Counts the words in a text and prints out the\n",
        "    resulting table in alphabetical order.\n",
        "    \"\"\"\n",
        "\n",
        "    # tokenizer separates words from punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # remove punctuation\n",
        "    words = [t.lower() for t in tokens if t.isalpha()]\n",
        "    \n",
        "    # get word counts from FreqDist\n",
        "    ordered = sorted(set([(w, v) for w, v in nltk.FreqDist(words).items()]))\n",
        "    \n",
        "    # get widths for pretty printing\n",
        "    # width of longest word\n",
        "    width = max([len(w) for w, _ in ordered]) + 2\n",
        "    # width of longest number\n",
        "    width_counts = max([len(str(v)) for _, v in ordered])\n",
        "    \n",
        "    # print everything\n",
        "    for w, v in ordered:\n",
        "        print(\"{}:{}{:>{}}\".format(w, ' ' * (width - len(w)), \n",
        "                                     v, width_counts))"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ4pFcP71Gu9",
        "outputId": "01affbab-d301-48e3-bf5c-2ff36ae6bfab"
      },
      "source": [
        "test = \"If police police police police, who polices the police police? \" \\\n",
        "       \"Police police police police police police.\"\n",
        "print_words_and_frequency(test)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "if:        1\n",
            "police:   12\n",
            "polices:   1\n",
            "the:       1\n",
            "who:       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SEQFmoc1e1r",
        "outputId": "7d3bf0a5-b059-4586-b200-08e86e5ba28e"
      },
      "source": [
        "test = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
        "print_words_and_frequency(test)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a:          2\n",
            "chuck:      2\n",
            "could:      1\n",
            "how:        1\n",
            "if:         1\n",
            "much:       1\n",
            "wood:       2\n",
            "woodchuck:  2\n",
            "would:      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeNga_ZAu00j"
      },
      "source": [
        "**16. Read up on Gematria, a method for assigning numbers to words, and for mapping between words having the same number to discover the hidden meaning of texts (http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).**\n",
        "\n",
        "**a.Write a function gematria() that sums the numerical values of the letters of a word, according to the letter values in letter_vals.**\n",
        "\n",
        "**b. Process a corpus (e.g. nltk.corpus.state_union ) and for each document, count how many of its words have the number 666.**\n",
        "\n",
        "**c. Write a function decode() to process a text, randomly replacing words with their Gematria equivalents, in order to discover the \"hidden meaning\" of the text.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro9uUXI8vNlE",
        "outputId": "feb51e94-10d0-47db-fdab-2527f5e71bd3"
      },
      "source": [
        "def gematria(a):\n",
        "  d=a.values()\n",
        "  total= sum(d)\n",
        "  print(\"Sum of Numerial Values : \",total)\n",
        "\n",
        "letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
        "'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
        "'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
        "\n",
        "gematria(letter_vals)\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of Numerial Values :  2305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVFZYsZxyk21"
      },
      "source": [
        "def gematria(word):\n",
        "    value = 0\n",
        "    letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
        "                  'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
        "                  'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
        "    for c in word:\n",
        "        if c in letter_vals:\n",
        "            value += letter_vals[c]\n",
        "    return value"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vna5Db8yugiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a557d927-812e-4ce7-d54d-47f0f9dfa69e"
      },
      "source": [
        "from nltk.corpus import state_union\n",
        "for fileid in state_union.fileids():\n",
        "    cnt = 0\n",
        "    for word in nltk.corpus.state_union.words(fileid):\n",
        "        if word.isalpha() and gematria(word.lower()) == 666:\n",
        "            cnt += 1\n",
        "    print(fileid, cnt)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1945-Truman.txt 2\n",
            "1946-Truman.txt 13\n",
            "1947-Truman.txt 0\n",
            "1948-Truman.txt 2\n",
            "1949-Truman.txt 2\n",
            "1950-Truman.txt 1\n",
            "1951-Truman.txt 0\n",
            "1953-Eisenhower.txt 1\n",
            "1954-Eisenhower.txt 6\n",
            "1955-Eisenhower.txt 3\n",
            "1956-Eisenhower.txt 1\n",
            "1957-Eisenhower.txt 2\n",
            "1958-Eisenhower.txt 5\n",
            "1959-Eisenhower.txt 1\n",
            "1960-Eisenhower.txt 5\n",
            "1961-Kennedy.txt 0\n",
            "1962-Kennedy.txt 11\n",
            "1963-Johnson.txt 0\n",
            "1963-Kennedy.txt 5\n",
            "1964-Johnson.txt 1\n",
            "1965-Johnson-1.txt 0\n",
            "1965-Johnson-2.txt 0\n",
            "1966-Johnson.txt 0\n",
            "1967-Johnson.txt 2\n",
            "1968-Johnson.txt 3\n",
            "1969-Johnson.txt 0\n",
            "1970-Nixon.txt 0\n",
            "1971-Nixon.txt 1\n",
            "1972-Nixon.txt 0\n",
            "1973-Nixon.txt 1\n",
            "1974-Nixon.txt 0\n",
            "1975-Ford.txt 0\n",
            "1976-Ford.txt 3\n",
            "1977-Ford.txt 0\n",
            "1978-Carter.txt 1\n",
            "1979-Carter.txt 2\n",
            "1980-Carter.txt 0\n",
            "1981-Reagan.txt 4\n",
            "1982-Reagan.txt 0\n",
            "1983-Reagan.txt 2\n",
            "1984-Reagan.txt 1\n",
            "1985-Reagan.txt 1\n",
            "1986-Reagan.txt 1\n",
            "1987-Reagan.txt 1\n",
            "1988-Reagan.txt 2\n",
            "1989-Bush.txt 1\n",
            "1990-Bush.txt 2\n",
            "1991-Bush-1.txt 0\n",
            "1991-Bush-2.txt 0\n",
            "1992-Bush.txt 3\n",
            "1993-Clinton.txt 1\n",
            "1994-Clinton.txt 2\n",
            "1995-Clinton.txt 1\n",
            "1996-Clinton.txt 2\n",
            "1997-Clinton.txt 1\n",
            "1998-Clinton.txt 4\n",
            "1999-Clinton.txt 1\n",
            "2000-Clinton.txt 3\n",
            "2001-GWBush-1.txt 1\n",
            "2001-GWBush-2.txt 0\n",
            "2002-GWBush.txt 0\n",
            "2003-GWBush.txt 3\n",
            "2004-GWBush.txt 2\n",
            "2005-GWBush.txt 2\n",
            "2006-GWBush.txt 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3VXU5e0vERJ"
      },
      "source": [
        "def decode(text):\n",
        "    gematrias = {}\n",
        "    splits = text.split()\n",
        "\n",
        "    # create dictionary \n",
        "    # key: gematria value\n",
        "    # value: list of words with that gematria value\n",
        "    for word in splits:\n",
        "        if gematria(word) in gematrias:\n",
        "            gematrias[gematria(word)].add(word)\n",
        "        else:\n",
        "            gematrias[gematria(word)] = [word]\n",
        "            \n",
        "    for i in range(len(splits)):\n",
        "        splits[i] = random.choice(gematrias[gematria(word)])\n",
        "    \n",
        "    return ' '.join(splits)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZBCJOcW1n6g"
      },
      "source": [
        "**17. Write a function shorten(text, n) to process a text, omitting the n most frequently occurring words of the text. How readable is it?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MA9-VcJ7yu5S",
        "outputId": "4686ce4e-493f-4145-8b6c-bbed02169ecd"
      },
      "source": [
        "def shorten(text, n):\n",
        "    splits = re.findall(r\"\\w+(?:[-']\\w+)*\", text)\n",
        "    fdist = nltk.FreqDist(splits)\n",
        "    # most_freq = fdist.most_common(n)\n",
        "    # print(most_freq)\n",
        "    most_freq = []\n",
        "    for sample in fdist.most_common(n):\n",
        "        most_freq.append(sample[0])\n",
        "    for i in range(len(splits)):\n",
        "        if splits[i] in most_freq:\n",
        "            splits[i] = ''                     # in fact, the element should be deleted in the list\n",
        "    return ' '.join(splits)\n",
        "\n",
        "text = '''Sonu Sood has shown his hidden talent of singing in this movie. He was unware of this talent and even his fans didn't know about it.'''\n",
        "shorten(text, 4)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Sonu Sood has shown  hidden   singing in  movie He was unware    and even  fans didn't know about it\""
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KupF3Ni7tQJ5"
      },
      "source": [
        "**18. Write code to print out an index for a lexicon, allowing someone to look up words according to their meanings (or pronunciations; whatever properties are contained in lexical entries).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aadObFDXtXWG",
        "outputId": "cb011d81-a15e-4da5-bd7a-18f09d4736db"
      },
      "source": [
        "def getWords(prop, value):\n",
        "    lexicon = [('fish', 'water animal', 'fish'), ('house', 'building', 'haus'), ('whale', 'water animal', 'wejl')]\n",
        "    if prop == 'meaning':\n",
        "        return [w for (w, m, p) in lexicon if m == value]\n",
        "    if prop == 'pronunciation':\n",
        "        return [w for (w, m, p) in lexicon if p == value]\n",
        "    \n",
        "getWords('meaning', 'water animal')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fish', 'whale']"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKEZ4ij3tbD9",
        "outputId": "1c2ad203-6548-4ee9-9c3c-09e29df77bde"
      },
      "source": [
        "getWords('pronunciation', 'haus')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['house']"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IDZ1Ui2RXI"
      },
      "source": [
        "**19. Write a list comprehension that sorts a list of WordNet synsets for proximity to a given synset. For example, given the synsets minke_whale.n.01, orca.n.01, novel.n.01 , and tortoise.n.01 , sort them according to their shortest_path_distance() from right_whale.n.01 .**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VaEQqVZ1v1c",
        "outputId": "9a93255a-5cc2-482c-e550-92ae25cc48f7"
      },
      "source": [
        "minke = wn.synset('minke_whale.n.01')\n",
        "orca = wn.synset('orca.n.01')\n",
        "novel = wn.synset('novel.n.01')\n",
        "tortoise = wn.synset('tortoise.n.01')\n",
        "right = wn.synset('right_whale.n.01')\n",
        "\n",
        "wn_list = [minke, orca, novel, tortoise]\n",
        "sorted(wn_list,\n",
        "       key=lambda x: right.shortest_path_distance(x))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('lesser_rorqual.n.01'),\n",
              " Synset('killer_whale.n.01'),\n",
              " Synset('tortoise.n.01'),\n",
              " Synset('novel.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2TfY2eo4Efi"
      },
      "source": [
        "**20. Write a function that takes a list of words (containing duplicates) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g. if the input list contained 10 instances of the word table and 9 instances of the word chair , then table would appear before chair in the output list.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTKxi--t2Y9d"
      },
      "source": [
        "def decreasing_freq_with_no_duplicates(words):\n",
        "    wordset = set(words)\n",
        "    fdist = nltk.FreqDist(words)\n",
        "    return sorted(wordset,\n",
        "                 key=lambda x:fdist[x],\n",
        "                 reverse=True)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3B9Zbd19tYO",
        "outputId": "1ab537c1-d00f-4cea-802b-8729d4d76213"
      },
      "source": [
        "words = ['Scooter'] * 10 + ['Car'] * 9\n",
        "decreasing_freq_with_no_duplicates(words)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Scooter', 'Car']"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adLkEoE2-DN8"
      },
      "source": [
        "**21. Write a function that takes a text and a vocabulary as its arguments and returns the set of words that appear in the text but not in the vocabulary. Both arguments can be represented as lists of strings. Can you do this in a single line, using set.difference() ?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDZrMNgD9v0O"
      },
      "source": [
        "def diff(text, vocab):\n",
        "    return set(text).difference(vocab)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2ppCfyH-H__",
        "outputId": "eb40d3c1-27c4-41e8-9bff-0bb87c5f7a17"
      },
      "source": [
        "text = 'a text and a vocabulary'.split()\n",
        "vocab = 'a vocabulary'.split()\n",
        "diff(text, vocab)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and', 'text'}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aq6DFVy_dDr"
      },
      "source": [
        "**22. Import the itemgetter() function from the operator module in Python's standard library (i.e. from operator import itemgetter ). Create a list words containing several words. Now try calling: sorted(words, key=itemgetter(1)) , and sorted(words, key=itemgetter(-1)) . Explain what itemgetter() is doing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS4nQkQ9-KW9",
        "outputId": "60ffc528-65fc-4394-ab80-cdc818f860ea"
      },
      "source": [
        "words = 'list wOrds containing several words'.split()\n",
        "sorted(words, key=itemgetter(1))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wOrds', 'several', 'list', 'containing', 'words']"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn_1hd3m_mYt"
      },
      "source": [
        "\n",
        "operator.itemgetter()\n",
        "\n",
        "operator.itemgetter() returns a callable object that fetches item from its operand using the operands __getitem__() method. If multiple items are specified, returns a tuple of lookup values. For example:\n",
        "\n",
        "After f = itemgetter(2), the call f(r) returns r[2].\n",
        "After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6RM9ya2A_q6"
      },
      "source": [
        "**23. Write a recursive function lookup(trie, key) that looks up a key in a trie, and returns the value it finds. Extend the function to return a word when it is uniquely determined by its prefix (e.g. vanguard is the only word that starts with vang-, so lookup(trie, 'vang') should return the same thing as lookup(trie, 'vanguard')).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRzsaLoi_i26"
      },
      "source": [
        "def insert(trie, key, value):\n",
        "    if key:\n",
        "        first, rest = key[0], key[1:]\n",
        "        if first not in trie:\n",
        "            trie[first] = {}\n",
        "        insert(trie[first], rest, value)\n",
        "    else:\n",
        "        trie['value'] = value"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtf0NzANBGaY",
        "outputId": "12296414-698e-45d6-ce2b-bc87fafb864e"
      },
      "source": [
        "from pprint import pprint\n",
        "trie = {}\n",
        "insert(trie, 'chat', 'cat')\n",
        "insert(trie, 'chien', 'dog')\n",
        "insert(trie, 'chair', 'flesh')\n",
        "insert(trie, 'chic', 'stylish')\n",
        "pprint(trie, width=40)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'c': {'h': {'a': {'i': {'r': {'value': 'flesh'}},\n",
            "                   't': {'value': 'cat'}},\n",
            "             'i': {'c': {'value': 'stylish'},\n",
            "                   'e': {'n': {'value': 'dog'}}}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYuyT14nBjkg"
      },
      "source": [
        "# original edition\n",
        "# must type the word completely\n",
        "def lookup(trie, key):\n",
        "    if key:\n",
        "        return lookup(trie[key[0]], key[1:])\n",
        "    else:\n",
        "        return trie['value']"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OY54GlEEBIQo",
        "outputId": "218ba609-dbca-4836-f818-aec4456161be"
      },
      "source": [
        "lookup(trie, 'chair')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'flesh'"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzi0-L-9BenX"
      },
      "source": [
        "# modification edition\n",
        "# can look up a word when it is uniquely determined by its prefix\n",
        "# (though the code is not elegant TAT\n",
        "def lookup1(trie, key):\n",
        "    if key:\n",
        "        return lookup1(trie[key[0]], key[1:])\n",
        "    else:\n",
        "        if 'value' in trie:\n",
        "            return trie['value']\n",
        "        if len(trie) == 1:\n",
        "            return lookup1(trie[list(trie)[0]], '')\n",
        "        else:\n",
        "            print('Invalid look up')"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSXKFblTBovC",
        "outputId": "b302a132-b671-4d61-88c6-4ab6413a69a5"
      },
      "source": [
        "insert(trie, 'vanguard', 'pioneer')\n",
        "lookup1(trie, 'vang') == lookup1(trie, 'vanguard')"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auqmikI9SKIO"
      },
      "source": [
        "**25. Read about string edit distance and the Levenshtein Algorithm. Try the implementation provided in nltk.edit_distance(). In what way is this using dynamic programming? Does it use the bottom-up or top-down approach? [See also http://norvig.com/spell-correct.html]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D6YCOEZSO5G",
        "outputId": "fb9f13c2-099e-464c-b430-e054aec2339c"
      },
      "source": [
        "help(nltk.edit_distance)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function edit_distance in module nltk.metrics.distance:\n",
            "\n",
            "edit_distance(s1, s2, substitution_cost=1, transpositions=False)\n",
            "    Calculate the Levenshtein edit-distance between two strings.\n",
            "    The edit distance is the number of characters that need to be\n",
            "    substituted, inserted, or deleted, to transform s1 into s2.  For\n",
            "    example, transforming \"rain\" to \"shine\" requires three steps,\n",
            "    consisting of two substitutions and one insertion:\n",
            "    \"rain\" -> \"sain\" -> \"shin\" -> \"shine\".  These operations could have\n",
            "    been done in other orders, but at least three steps are needed.\n",
            "    \n",
            "    Allows specifying the cost of substitution edits (e.g., \"a\" -> \"b\"),\n",
            "    because sometimes it makes sense to assign greater penalties to substitutions.\n",
            "    \n",
            "    This also optionally allows transposition edits (e.g., \"ab\" -> \"ba\"),\n",
            "    though this is disabled by default.\n",
            "    \n",
            "    :param s1, s2: The strings to be analysed\n",
            "    :param transpositions: Whether to allow transposition edits\n",
            "    :type s1: str\n",
            "    :type s2: str\n",
            "    :type substitution_cost: int\n",
            "    :type transpositions: bool\n",
            "    :rtype int\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ofnp6hlSXCh",
        "outputId": "e6801ac1-a097-467f-e3ed-bf2debed2016"
      },
      "source": [
        "nltk.edit_distance('water', 'wine')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nek5VsrNSabu",
        "outputId": "e3491c72-51b3-4075-c86e-3e343fae70e5"
      },
      "source": [
        "nltk.edit_distance('cat', 'dog')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJvR-UePSteZ"
      },
      "source": [
        "**26.The Catalan numbers arise in many applications of combinatorial mathematics, including the counting of parse trees. The series can be defined as follows: $C_0 = 1$, and $C_{n+1} = \\sum_{0..n} (C_iC_{n-i})$.**\n",
        "\n",
        "**a. Write a recursive function to compute $n$th Catalan number $C_n$.**\n",
        "\n",
        "**b. Now write another function that does this computation using dynamic programming.**\n",
        "\n",
        "**c. Use the timeit module to compare the performance of these functions as n increases.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfB95lSdS_EG"
      },
      "source": [
        "def Catalan(n):\n",
        "    if n < 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIEqJgVNTEFW",
        "outputId": "a7867d18-c039-4bcc-b9a1-39f089b702e4"
      },
      "source": [
        "Catalan(11)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58786"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcHCHLG7TGdd"
      },
      "source": [
        "def Catalan_memo(n, lookup = {0: 1}):\n",
        "    if n not in lookup:\n",
        "        result = sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n",
        "        lookup[n] = result\n",
        "    return lookup[n]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlxy1CuGTIwc",
        "outputId": "66eab120-13d0-43d6-a41a-f91c44625714"
      },
      "source": [
        "Catalan_memo(11)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58786"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdEmn0eMTL_z"
      },
      "source": [
        "from nltk import memoize\n",
        "@memoize\n",
        "\n",
        "def Catalan_memoize(n):\n",
        "    if n < 1:\n",
        "        return 1\n",
        "    else:\n",
        "        result = sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n",
        "        return result"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv0i52SRTQQ1",
        "outputId": "f0612c5b-c355-4936-f09a-1b6b41ce35e0"
      },
      "source": [
        "Catalan_memoize(11)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58786"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Es0v_cTXP5",
        "outputId": "fecccab0-798a-42e7-84c8-a28e45260346"
      },
      "source": [
        "%%time\n",
        "print([Catalan(i) for i in range(15)])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440]\n",
            "CPU times: user 2.91 s, sys: 5.12 ms, total: 2.92 s\n",
            "Wall time: 2.97 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtlPa79OTZSf",
        "outputId": "6380d355-2b8a-4978-f0d3-57b424e1c5dd"
      },
      "source": [
        "%%time\n",
        "print([Catalan(i) for i in range(15)])"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440]\n",
            "CPU times: user 2.88 s, sys: 3.39 ms, total: 2.89 s\n",
            "Wall time: 2.89 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFJHg8kTaHr",
        "outputId": "c2d70bb9-d3b0-4f5d-b716-56656441c17a"
      },
      "source": [
        "%%time\n",
        "print([Catalan(i) for i in range(15)])"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440]\n",
            "CPU times: user 2.92 s, sys: 9.32 ms, total: 2.92 s\n",
            "Wall time: 2.96 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4IPYTVbTc9v"
      },
      "source": [
        "def pr(defs, results):\n",
        "    fmt = '%-16s ' * 3\n",
        "    print((fmt % tuple(c.__name__ for c in defs)).upper())\n",
        "    print(fmt % ((\"=\" * 16,) * 3))\n",
        "    for r in zip(*results):\n",
        "        print(fmt % r)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X89zhjG2Tdy7",
        "outputId": "69d56a3d-7d59-4152-e280-3c73e383ebb8"
      },
      "source": [
        "defs = (Catalan, Catalan_memo, Catalan_memoize)\n",
        "results = [tuple(c(i) for i in range(15)) for c in defs]\n",
        "pr(defs, results)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CATALAN          CATALAN_MEMO     CATALAN_MEMOIZE  \n",
            "================ ================ ================ \n",
            "1                1                1                \n",
            "1                1                1                \n",
            "2                2                2                \n",
            "5                5                5                \n",
            "14               14               14               \n",
            "42               42               42               \n",
            "132              132              132              \n",
            "429              429              429              \n",
            "1430             1430             1430             \n",
            "4862             4862             4862             \n",
            "16796            16796            16796            \n",
            "58786            58786            58786            \n",
            "208012           208012           208012           \n",
            "742900           742900           742900           \n",
            "2674440          2674440          2674440          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzIzfreDR3ht"
      },
      "source": [
        "**29. Write a recursive function that pretty prints a trie in alphabetically sorted order, e.g.:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJZ3Kh4gBq62",
        "outputId": "64fe468f-e205-4f78-a283-0c7c4072e607"
      },
      "source": [
        "def insert(trie, key, value):\n",
        "    \"\"\"Insert into Trie\"\"\"\n",
        "    if key:\n",
        "        first, rest = key[0], key[1:]\n",
        "        if first not in trie:\n",
        "            trie[first] = {}\n",
        "        insert(trie[first], rest, value)\n",
        "    else:\n",
        "        trie['value'] = value\n",
        "\n",
        "def display(trie, s = \"\"):\n",
        "  \"\"\"Recursive function to Display Trie entries in alphabetical order\"\"\"\n",
        "  first = True\n",
        "  for k, v in sorted(trie.items(), key = lambda x: x[0]):\n",
        "    # dictionary sorted based upon the keys\n",
        "    if isinstance(v, dict):\n",
        "      if first:\n",
        "        prefix = s + k          # first to show common prefix\n",
        "        first = False\n",
        "      else:\n",
        "        prefix = '-'*len(s) + k  # dashes for common prefix\n",
        "\n",
        "      display(v, prefix)   # s+k is extending string s for display by appending current key k\n",
        "    else:\n",
        "      print(s, \":\", v)  # not a dictionary, so print current   # not a dictionary, so print current string s and value\n",
        "\n",
        "# Create Trie\n",
        "trie = {}\n",
        "insert(trie, 'chat', 'cat')\n",
        "insert(trie, 'chien', 'dog')\n",
        "insert(trie, 'chair', 'flesh')\n",
        "insert(trie, 'chic', 'stylish')\n",
        "\n",
        "#Display Use Recursive function (second argument will default to \"\" on call)\n",
        "display(trie)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chair : flesh\n",
            "---t : cat\n",
            "--ic : stylish\n",
            "---en : dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyYkjbhCj82w"
      },
      "source": [
        "**30. With the help of the trie data structure, write a recursive function that processes text, locating the uniqueness point in each word, and discarding the remainder of each word. How much compression does this give? How readable is the resulting text?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5e4MQLikHgk"
      },
      "source": [
        "def lookup_unique(key, trie, unique='', buffer_unique=''):\n",
        "    if len(key) == 0:\n",
        "        if len(buffer_unique) > 0:\n",
        "            return buffer_unique\n",
        "        else:  \n",
        "            return unique\n",
        "    if len(trie[key[0]]) == 1:\n",
        "        if len(buffer_unique) > 0:\n",
        "            new_buffer_unique = buffer_unique\n",
        "        else:\n",
        "            new_buffer_unique = unique + key[0]\n",
        "        return lookup_unique(key[1:], trie[key[0]], unique + key[0], new_buffer_unique)\n",
        "    return lookup_unique(key[1:], trie[key[0]], unique + key[0])\n",
        "        \n",
        "\n",
        "def compress(text):          \n",
        "    trie = nltk.defaultdict(dict)\n",
        "    for word in text:\n",
        "        insert(trie, word, word)\n",
        "    return [lookup_unique(w, trie) for w in text]\n",
        "\n",
        "compressed = compress(text1)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BpcY_rwkzuJ",
        "outputId": "9ff129b0-b98d-4d56-dc53-322afc127a2d"
      },
      "source": [
        "from __future__ import division\n",
        "length = len(text1)\n",
        "a1 = 100.0/length \n",
        "b= len(''.join(compressed))\n",
        "print(a1*b)\n",
        "print (' '.join(compressed[:200]))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2200.0\n",
            "Hush, Papa'sgoingtobuyyouamockingbird. Ifthatmockingbird Papa'sgoingtobuyyouadiamond Ifthatdiamond Papa'sgoingtobuyyoualooking-glass. Ifthatlooking-glass Papa'sgoingtobuyyouabilly-goat. Ifthatbilly-goat Papa'sgoingtobuyyouanother\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khALBiWKC3_I"
      },
      "source": [
        "**31. Obtain some raw text, in the form of a single, long string. Use Python's textwrap module to break it up into multiple lines. Now write code to add extra spaces between words, in order to justify the output. Each line must have the same width, and spaces must be approximately evenly distributed across each line. No line can begin or end with a space.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrTaJca3CQF2",
        "outputId": "21214435-96a7-4bde-dcc8-4f9bc764ed62"
      },
      "source": [
        "import textwrap\n",
        "#add extra spaces\n",
        "a = \"A trie is a tree-like data structure whose nodes store the letters of an alphabet.By structuring the nodes in a particular way, words and strings can be retrieved from \\\n",
        "the structure by traversing down a branch path of the tree. Tries in the context of computer science are a relatively new thing.\"\n",
        "b= textwrap.fill(a)\n",
        "print(\"\".join(b))\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A trie is a tree-like data structure whose nodes store the letters of\n",
            "an alphabet.By structuring the nodes in a particular way, words and\n",
            "strings can be retrieved from the structure by traversing down a\n",
            "branch path of the tree. Tries in the context of computer science are\n",
            "a relatively new thing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5AJm_jbD_Wg",
        "outputId": "93464e8d-a760-4aea-a573-aeee4de6bb23"
      },
      "source": [
        "#same width\n",
        "c= textwrap.dedent(a).strip()\n",
        "print(\"\".join(b))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A trie is a tree-like data structure whose nodes store the letters of\n",
            "an alphabet.By structuring the nodes in a particular way, words and\n",
            "strings can be retrieved from the structure by traversing down a\n",
            "branch path of the tree. Tries in the context of computer science are\n",
            "a relatively new thing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MacuaAACWseZ"
      },
      "source": [
        "**32. Develop a simple extractive summarization tool, that prints the sentences of a document which contain the highest total word frequency. Use FreqDist() to count word frequencies, and use sum to sum the frequencies of the words in each sentence. Rank the sentences according to their score. Finally, print the n highest-scoring sentences in document order. Carefully review the design of your program, especially your approach to this double sorting. Make sure the program is written as clearly as possible.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjS6hebtYy-A",
        "outputId": "cb311763-a672-4e7d-8bfa-e0bbf8b1d822"
      },
      "source": [
        "import math\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text_str = '''\n",
        "Breaking Bad is an American neo-Western crime drama television series created and produced by Vince Gilligan. \n",
        "The show aired on AMC from January 20, 2008, to September 29, 2013, consisting of five seasons for a total of 62 episodes. \n",
        "It was set and filmed in Albuquerque, New Mexico, and tells the story of Walter White (Bryan Cranston), an underpaid, overqualified, \n",
        "and dispirited high school chemistry teacher who is struggling with a recent diagnosis of stage-three lung cancer. White turns to a life of crime, \n",
        "partnering with his former student Jesse Pinkman (Aaron Paul), by producing and distributing crystal meth to secure his family's financial future \n",
        "before he dies,while navigating the dangers of the criminal underworld. According to Gilligan, the title is a Southern colloquialism meaning \n",
        "\"to raise hell\".Among the show's co-stars are Anna Gunn and RJ Mitte as Walter's wife Skyler and son Walter Jr., and Betsy Brandt and Dean \n",
        "Norris as Skyler's sister Marie Schrader and her husband Hank, a DEA agent. Others include Bob Odenkirk as White's and Pinkman's sleazy lawyer \n",
        "Saul Goodman, Jonathan Banks as private investigator and fixer Mike Ehrmantraut, and Giancarlo Esposito as drug kingpin Gus Fring. \n",
        "The final season introduces Jesse Plemons as the criminally ambitious Todd Alquist, and Laura Fraser as Lydia Rodarte-Quayle,\n",
        "a cunning business executive secretly managing Walter's global meth sales for her company.Breaking Bad's first season received \n",
        "generally positive reviews,while the rest of its run received universal acclaim. Since its conclusion, the show has been lauded by \n",
        "critics as one of the greatest television series of all time.[9] It had fair viewership in its first three seasons, but the fourth and \n",
        "fifth seasons saw a moderate rise in viewership when it was made available on Netflix just before the fourth season premiere. \n",
        "Viewership increased more drastically upon the premiere of the second half of the fifth season in 2013. By the time that the series finale aired, \n",
        "it was among the most-watched cable shows on American television. The show received numerous awards, including 16 Primetime Emmy Awards, eight \n",
        "Satellite Awards, two Golden Globe Awards, two Peabody Awards, two Critics' Choice Awards, and four Television Critics Association Awards. \n",
        "Cranston won the Primetime Emmy Award for Outstanding Lead Actor in a Drama Series four times, while Aaron Paul won the Primetime \n",
        "Emmy Award for Outstanding Supporting Actor in a Drama Series three times; Anna Gunn won the Primetime Emmy Award for Outstanding \n",
        "Supporting Actress in a Drama Series twice. \n",
        "In 2013, Breaking Bad entered the Guinness World Records as the most critically acclaimed show of all-time.\n",
        "Better Call Saul, a prequel series featuring Odenkirk, Banks, and Esposito reprising their Breaking Bad roles, \n",
        "debuted on AMC on February 8, 2015, and has been renewed for a final sixth season. The sequel film El Camino: \n",
        "A Breaking Bad Movie starring Aaron Paul was released on Netflix and in theaters on October 11, 2019.'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _create_frequency_table(text_string) -> dict:\n",
        "    \"\"\"\n",
        "    we create a dictionary for the word frequency table.\n",
        "    For this, we should only use the words that are not part of the stopWords array.\n",
        "    Removing stop words and making frequency table\n",
        "    Stemmer - an algorithm to bring words to its root word.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable\n",
        "\n",
        "\n",
        "def _create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return frequency_matrix\n",
        "\n",
        "\n",
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "def _create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table\n",
        "\n",
        "\n",
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "\n",
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "def _score_sentences(tf_idf_matrix) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its word's TF\n",
        "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n",
        "\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def run_summarization(text):\n",
        "    \"\"\"\n",
        "    :param text: Plain summary_text of long article\n",
        "    :return: summarized summary_text\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    We already have a sentence tokenizer, so we just need \n",
        "    to run the sent_tokenize() method to create the array of sentences.\n",
        "    '''\n",
        "    # 1 Sentence Tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_documents = len(sentences)\n",
        "    #print(sentences)\n",
        "\n",
        "    # 2 Create the Frequency matrix of the words in each sentence.\n",
        "    freq_matrix = _create_frequency_matrix(sentences)\n",
        "    #print(freq_matrix)\n",
        "\n",
        "    '''\n",
        "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
        "    '''\n",
        "    # 3 Calculate TermFrequency and generate a matrix\n",
        "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "    #print(tf_matrix)\n",
        "\n",
        "    # 4 creating table for documents per words\n",
        "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "    #print(count_doc_per_words)\n",
        "\n",
        "    '''\n",
        "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
        "    '''\n",
        "    # 5 Calculate IDF and generate a matrix\n",
        "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "    #print(idf_matrix)\n",
        "\n",
        "    # 6 Calculate TF-IDF and generate a matrix\n",
        "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "    #print(tf_idf_matrix)\n",
        "\n",
        "    # 7 Important Algorithm: score the sentences\n",
        "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "    #print(sentence_scores)\n",
        "\n",
        "    # 8 Find the threshold\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "    #print(threshold)\n",
        "\n",
        "    # 9 Important Algorithm: Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "    return summary\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    result = run_summarization(text_str)\n",
        "    print(result)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Viewership increased more drastically upon the premiere of the second half of the fifth season in 2013. Cranston won the Primetime Emmy Award for Outstanding Lead Actor in a Drama Series four times, while Aaron Paul won the Primetime \n",
            "Emmy Award for Outstanding Supporting Actor in a Drama Series three times; Anna Gunn won the Primetime Emmy Award for Outstanding \n",
            "Supporting Actress in a Drama Series twice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oXnKOC2mhND"
      },
      "source": [
        "**35. Write a program to implement a brute-force algorithm for discovering word squares, a kind of n  n crossword in which the entry in the nth row is the same as the entry in the nth column. For discussion, see http://itre.cis.upenn.edu/~myl/languagelog/archives/002679.html**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8ZkF9PGZiNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3486ba-e6f0-4345-c340-0672948d7fd6"
      },
      "source": [
        "def word_square(n):\n",
        "    # works only if n < 5, with 5 exceeds maximum recursion callstack\n",
        "    # TODO: Do this iteratively to avoid the callstack issue?\n",
        "    from nltk.corpus import words\n",
        "    myWords = [word.upper() for word in filter(lambda w: len(w) == n, words.words())] # get all words of length n\n",
        "    \n",
        "    square = []\n",
        "    skipWords = [[] for i in range(n)] # cache for words that have already been tested at position i\n",
        "    \n",
        "    def check_against_square(word): # checks if current state of square would allow to add word to it\n",
        "        if word in square:\n",
        "            return False\n",
        "        for (index, square_word) in enumerate(square):\n",
        "            if (word[index] != square_word[len(square)]):\n",
        "                return False\n",
        "        return True\n",
        "    \n",
        "    def add_word(): # recursively adds / removes words from square until solution is found\n",
        "        if len(square) == n:\n",
        "            return True\n",
        "        for word in myWords:\n",
        "            if len(square) == n:\n",
        "                return True\n",
        "            if (word not in skipWords[len(square)]) and check_against_square(word): # add the word to square if it hasn't been tested unsuccessfully already and if it fits \n",
        "                square.append(word)\n",
        "                add_word()\n",
        "        if len(square) != n and len(square) != 0:   \n",
        "            skipWords[len(square) - 1].append(square.pop()) # add word to cache\n",
        "            for i in range(len(square) + 1, n): # reset the following parts of the cache\n",
        "                skipWords[i] = []\n",
        "            add_word()\n",
        "        return False\n",
        "            \n",
        "        \n",
        "    if add_word():\n",
        "        for word in square:\n",
        "            print (word)\n",
        "    else:\n",
        "        print ('No square found :/')\n",
        "            \n",
        "word_square(4)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AANI\n",
            "ABAC\n",
            "NACE\n",
            "ICED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEz3xxXymgsx",
        "outputId": "fd60e093-0a94-4ef1-8dfa-c1f058a0721b"
      },
      "source": [
        "word_square(3)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AAL\n",
            "ABA\n",
            "LAB\n"
          ]
        }
      ]
    }
  ]
}